{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confirmed-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "great-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Don’t be disheartened, try and try until you succeed! To resubmit your project or view other projects go to your projects dashboard.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-barrier",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the first process of NLP. it is used to break the text into list of words or sentances for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-request",
   "metadata": {},
   "source": [
    "### 1. word_tokenize\n",
    "\n",
    "word_tokenize create list of words based on spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "finite-decade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don',\n",
       " '’',\n",
       " 't',\n",
       " 'be',\n",
       " 'disheartened',\n",
       " ',',\n",
       " 'try',\n",
       " 'and',\n",
       " 'try',\n",
       " 'until',\n",
       " 'you',\n",
       " 'succeed',\n",
       " '!',\n",
       " 'To',\n",
       " 'resubmit',\n",
       " 'your',\n",
       " 'project',\n",
       " 'or',\n",
       " 'view',\n",
       " 'other',\n",
       " 'projects',\n",
       " 'go',\n",
       " 'to',\n",
       " 'your',\n",
       " 'projects',\n",
       " 'dashboard',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-georgia",
   "metadata": {},
   "source": [
    "### 2. sent_tokenize\n",
    "sent_tokenize is Sentance Tokenizer which break peragraph into sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "welcome-cricket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don’t be disheartened, try and try until you succeed!',\n",
       " 'To resubmit your project or view other projects go to your projects dashboard.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = sent_tokenize(sentence)\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-community",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\n",
    "\n",
    "It is the 2nd stage of NLP. We use stemming on tokenize words to convert words into root words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-shanghai",
   "metadata": {},
   "source": [
    "### 1. PorterStammer\n",
    "\n",
    "PorterStemmer being the oldest one originally developed in 1979. PorterStemmer uses Suffix Stripping to produce stems. **PorterStemmer algorithm does not follow linguistics rather a set of 05 rules for different cases that are applied in phases (step by step) to generate stems**. This is the reason why PorterStemmer does not often generate stems that are actual English words. It does not keep a lookup table for actual stems of the word but applies algorithmic rules to generate stems. It uses the rules to decide whether it is wise to strip a suffix. One can generate its own set of rules for any language that is why Python nltk introduced SnowballStemmers that are used to create non-English Stemmers!\n",
    "\n",
    "\n",
    "**PorterStemmer is known for its simplicity and speed**. It is commonly useful in **Information Retrieval Environments** known as **IR Environments** for fast recall and fetching of search queries. In a typical IR, environment documents are represented as vectors of words or terms. Words having the same stem will have a similar meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nonprofit-howard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "don\n",
      "’\n",
      "t\n",
      "be\n",
      "dishearten\n",
      ",\n",
      "tri\n",
      "and\n",
      "tri\n",
      "until\n",
      "you\n",
      "succeed\n",
      "!\n",
      "To\n",
      "resubmit\n",
      "your\n",
      "project\n",
      "or\n",
      "view\n",
      "other\n",
      "project\n",
      "go\n",
      "to\n",
      "your\n",
      "project\n",
      "dashboard\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    rootWord = ps.stem(word)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-awareness",
   "metadata": {},
   "source": [
    "### 2. LancasterStemmer (Paice-Husk Stemmer)\n",
    "\n",
    "LancasterStemmer was developed in 1990 and uses a more aggressive approach than Porter Stemming Algorithm. The LancasterStemmer (Paice-Husk stemmer) is an iterative algorithm with rules saved externally. One table containing about 120 rules indexed by the last letter of a suffix. On each iteration, it tries to find an applicable rule by the last character of the word. Each rule specifies either a deletion or replacement of an ending. If there is no such rule, it terminates. It also terminates if a word starts with a vowel and there are only two letters left or if a word starts with a consonant and there are only three characters left. Otherwise, the rule is applied, and the process repeats.\n",
    "\n",
    "**LancasterStemmer is simple, but heavy stemming due to iterations and over-stemming may occur. Over-stemming causes the stems to be not linguistic, or they may have no meaning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "contained-killer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "don\n",
      "’\n",
      "t\n",
      "be\n",
      "disheart\n",
      ",\n",
      "try\n",
      "and\n",
      "try\n",
      "until\n",
      "you\n",
      "success\n",
      "!\n",
      "to\n",
      "resubmit\n",
      "yo\n",
      "project\n",
      "or\n",
      "view\n",
      "oth\n",
      "project\n",
      "go\n",
      "to\n",
      "yo\n",
      "project\n",
      "dashboard\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "ls = LancasterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    rootWord = ls.stem(word)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-cutting",
   "metadata": {},
   "source": [
    "## 3. SnowballStemmers\n",
    "\n",
    "This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer. That being said, it is also more aggressive than the Porter stemmer. A lot of the things added to the Snowball stemmer were because of issues noticed with the Porter stemmer. There is about a 5% difference in the way that Snowball stems versus Porter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "historical-motion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic, danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, porter, portuguese, romanian, russian, spanish, swedish\n"
     ]
    }
   ],
   "source": [
    "# See which languages are supported\n",
    "print(\", \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "electrical-visitor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "don\n",
      "’\n",
      "t\n",
      "be\n",
      "dishearten\n",
      ",\n",
      "tri\n",
      "and\n",
      "tri\n",
      "until\n",
      "you\n",
      "succeed\n",
      "!\n",
      "to\n",
      "resubmit\n",
      "your\n",
      "project\n",
      "or\n",
      "view\n",
      "other\n",
      "project\n",
      "go\n",
      "to\n",
      "your\n",
      "project\n",
      "dashboard\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "for word in words:\n",
    "    rootWord = stemmer.stem(word)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-worthy",
   "metadata": {},
   "source": [
    "## Compearing all Stemming results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gorgeous-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 PorterStemmer        LancasterStemmer     SnowballStemmer     \n",
      "-----------------------------------------------------------------------------------\n",
      "friend               friend               friend               friend              \n",
      "friendship           friendship           friend               friendship          \n",
      "friends              friend               friend               friend              \n",
      "friendships          friendship           friend               friendship          \n",
      "stabil               stabil               stabl                stabil              \n",
      "destabilize          destabil             dest                 destabil            \n",
      "misunderstanding     misunderstand        misunderstand        misunderstand       \n",
      "railroad             railroad             railroad             railroad            \n",
      "moonlight            moonlight            moonlight            moonlight           \n",
      "football             footbal              footbal              footbal             \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {3:20}\".format(\"Word\",\"PorterStemmer\",\"LancasterStemmer\",\"SnowballStemmer\"))\n",
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {3:20}\".format(word, ps.stem(word), ls.stem(word), stemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-belize",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "lemmatization to resolve a word to its lemma, it needs to know its part of speech. That requires extra computational linguistics power such as a part of speech tagger. This allows it to do better resolutions (like resolving is and are to “be”). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-disease",
   "metadata": {},
   "source": [
    "## 1. WordNetLemmatizer\n",
    "\n",
    "WordNet Lemmatizer uses the WordNet Database to lookup lemmas of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "equipped-market",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                 lemma               \n",
      "--------------------------------------\n",
      "The                  The                 \n",
      "striped              striped             \n",
      "bats                 bat                 \n",
      "are                  are                 \n",
      "hanging              hanging             \n",
      "on                   on                  \n",
      "their                their               \n",
      "feet                 foot                \n",
      "for                  for                 \n",
      "best                 best                \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(\"{0:20} {1:20}\".format(\"word\", \"lemma\"))\n",
    "print(\"--------------------------------------\")\n",
    "for word in words:\n",
    "    rootWord = wnl.lemmatize(word)\n",
    "    print(\"{0:20} {1:20}\".format(word, rootWord))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-waters",
   "metadata": {},
   "source": [
    "it didn’t do a good job. Because, ‘are’ is not converted to ‘be’ and ‘hanging’ is not converted to ‘hang’ as expected. This can be corrected if we provide the correct ‘part-of-speech’ tag (POS tag) as the second argument to lemmatize()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-explanation",
   "metadata": {},
   "source": [
    "In the above output, no actual root form has been given for any word, this is because they are given without context. We need to provide the context in which we want to lemmatize that is the parts-of-speech (POS). This is done by giving the value for pos parameter in wordnet_lemmatizer.lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "egyptian-belly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                 lemma               \n",
      "--------------------------------------\n",
      "The                  The                 \n",
      "striped              strip               \n",
      "bats                 bat                 \n",
      "are                  be                  \n",
      "hanging              hang                \n",
      "on                   on                  \n",
      "their                their               \n",
      "feet                 feet                \n",
      "for                  for                 \n",
      "best                 best                \n"
     ]
    }
   ],
   "source": [
    "print(\"{0:20} {1:20}\".format(\"word\", \"lemma\"))\n",
    "print(\"--------------------------------------\")\n",
    "for word in words:\n",
    "    rootWord = wnl.lemmatize(word, pos='v')\n",
    "    print(\"{0:20} {1:20}\".format(word, rootWord))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-timothy",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS) tagging\n",
    "\n",
    "It is a process of converting a sentence to forms – list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "funded-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "colonial-economy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('striped', 'JJ'),\n",
       " ('bats', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('hanging', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('their', 'PRP$'),\n",
       " ('feet', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('best', 'JJS')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posTag = pos_tag(words)\n",
    "posTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "equipped-saint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('striped', 'ADJ'),\n",
       " ('bats', 'NOUN'),\n",
       " ('are', 'VERB'),\n",
       " ('hanging', 'VERB'),\n",
       " ('on', 'ADP'),\n",
       " ('their', 'PRON'),\n",
       " ('feet', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('best', 'ADJ')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universalPosTag = pos_tag(words, tagset='universal')\n",
    "universalPosTag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-parks",
   "metadata": {},
   "source": [
    "### passing custom pos_tag to the lemmatizer\n",
    "\n",
    "nltk.pos_tag() returns a tuple with the POS tag. The key here is to map NLTK’s POS tags to the format wordnet lemmatizer would accept. The get_wordnet_pos() function defined below does this mapping job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "guilty-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "               \"N\": wordnet.NOUN,\n",
    "               \"V\": wordnet.VERB,\n",
    "               \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "finite-affect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "print([wnl.lemmatize(w, get_wordnet_pos(w)) for w in word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-solid",
   "metadata": {},
   "source": [
    "## 2. spaCy Lemmatizer\n",
    "\n",
    "spaCy is a relatively new in the space and is billed as an industrial strength NLP engine. It comes with pre-built models that can parse text and compute various NLP related features through one single function call. Ofcourse, it provides the lemma of the word too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sixth-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spaCy\n",
    "# !spaCy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-lying",
   "metadata": {},
   "source": [
    "spaCy determines the part-of-speech tag by default and assigns the corresponding lemma. It comes with a bunch of prebuilt models where the ‘en’ we just downloaded above is one of the standard ones for english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ranging-catholic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the stripe bat be hang on their foot for good'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-transparency",
   "metadata": {},
   "source": [
    "It did all the lemmatizations the Wordnet Lemmatizer supplied with the correct POS tag did. Plus it also lemmatized ‘best’ to ‘good’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-assembly",
   "metadata": {},
   "source": [
    "## 3. TextBlob Lemmatizer\n",
    "\n",
    "TexxtBlob is a powerful, fast and convenient NLP package as well. Using the Word and TextBlob objects, its quite straighforward to parse and lemmatize words and sentences respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "collected-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "loved-courage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stripe'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob,Word\n",
    "\n",
    "# Lemmatize a word\n",
    "word = 'stripes'\n",
    "w = Word(word)\n",
    "w.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-search",
   "metadata": {},
   "source": [
    "However to lemmatize a sentence or paragraph, we parse it using TextBlob and call the lemmatize() function on the parsed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "false-reputation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The striped bat are hanging on their foot for best'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize a sentence\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "sent = TextBlob(sentence)\n",
    "\" \". join([w.lemmatize() for w in sent.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-contact",
   "metadata": {},
   "source": [
    "It did not do a great job at the outset, because, like NLTK, TextBlob also uses wordnet internally. So, let’s pass the appropriate POS tag to the lemmatize() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-indicator",
   "metadata": {},
   "source": [
    "### TextBlob Lemmatizer with appropriate POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "american-advertiser",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The striped bat be hang on their foot for best'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define function to lemmatize each word with its POS tag\n",
    "\n",
    "def lemmatize_with_postag(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w,pos in sent.tags]\n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "# Lemmatize\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "lemmatize_with_postag(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-incident",
   "metadata": {},
   "source": [
    "## 4. Pattern Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "determined-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "engaged-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pattern\n",
    "from pattern.en import lemma, lexeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "unexpected-society",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the stripe bat be hang on their feet and eat best fishes'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not sure why but its always throwing error on first execution\n",
    "sent = \"The striped bats were hanging on their feet and ate best fishes\"\n",
    "\n",
    "\" \".join([lemma(wd) for wd in sent.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "monetary-checklist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'thes', 'thing', 'thed'],\n",
       " ['stripe', 'stripes', 'striping', 'striped'],\n",
       " ['bat', 'bats', 'batting', 'batted'],\n",
       " ['be',\n",
       "  'am',\n",
       "  'are',\n",
       "  'is',\n",
       "  'being',\n",
       "  'was',\n",
       "  'were',\n",
       "  'been',\n",
       "  'am not',\n",
       "  \"aren't\",\n",
       "  \"isn't\",\n",
       "  \"wasn't\",\n",
       "  \"weren't\"],\n",
       " ['hang', 'hangs', 'hanging', 'hung'],\n",
       " ['on', 'ons', 'oning', 'oned'],\n",
       " ['their', 'theirs', 'theiring', 'theired'],\n",
       " ['feet', 'feets', 'feeting', 'feeted'],\n",
       " ['for', 'fors', 'forring', 'forred'],\n",
       " ['best', 'bests', 'besting', 'bested']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also view the possible lexeme’s for each word.\n",
    "\n",
    "[lexeme(wd) for wd in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "universal-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also obtain the lemma by parsing the text.\n",
    "\n",
    "from pattern.en import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adjacent-friday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The/DT/the striped/JJ/striped bats/NNS/bat were/VBD/be hanging/VBG/hang on/IN/on their/PRP$/their feet/NNS/foot and/CC/and ate/VBD/eat best/JJ/best fishes/NNS/fish\n"
     ]
    }
   ],
   "source": [
    "print(parse('The striped bats were hanging on their feet and ate best fishes', \n",
    "            lemmata=True, tags=False, chunks=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-violence",
   "metadata": {},
   "source": [
    "## Applications of Stemming and Lemmatization\n",
    "Stemming and Lemmatization are itself form of NLP and widely used in Text mining. Text Mining is the process of analysis of texts written in natural language and extract high-quality information from text. It involves looking for interesting patterns in the text or to extract data from the text to be inserted into a database. Text mining tasks include **text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities)**. Developers have to prepare text using lexical analysis, POS (Parts-of-speech) tagging, stemming and other Natural Language Processing techniques to gain useful information from text.\n",
    "\n",
    "#### Information Retrieval (IR) Environments:\n",
    "It is useful to use stemming and lemmatization to map documents to common topics and display search results by indexing when documents are increasing to mind-boggling numbers. **Query Expansion** is a term used in Search Environments which refers to that when a user inputs a query. It is used to expand or enhance the query to match additional documents.\n",
    "Stemming has been used in Query systems such as Web Search Engines, but due to problems of under-stemming and over-stemming it's effectiveness in returning correct results have been found limited. For example, a person searching for 'marketing' may not be pleased with results that will show 'markets' and not marketing. But Stemming may be found useful in other languages and using different algorithms for stemming may result in better outputs. Google search adopted stemming in 2003.\n",
    "\n",
    "#### Sentiment Analysis\n",
    "Sentiment Analysis is the analysis of people's reviews and comments about something. It is widely used for analysis of product on online retail shops. Stemming and Lemmatization is used as part of the text-preparation process before it is analyzed.\n",
    "\n",
    "#### Document Clustering\n",
    "Document clustering (or text clustering) is the application of cluster analysis to textual documents. It has applications in an automatic document organization, topic extraction, and fast information retrieval or filtering. Examples of document clustering include web document clustering for search engines. Before Clustering methods are applied document is prepared through tokenization, removal of stop words and then Stemming and Lemmatization to reduce the number of tokens that carry out the same information and hence speed up the whole process. After this pre-processing, features are calculated by calculating the frequency of all tokens and then clustering methods are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-absence",
   "metadata": {},
   "source": [
    "## Stemming or lemmatization?\n",
    "\n",
    "1. Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "2. Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma.\n",
    "\n",
    "So when to use what! The above points show that if speed is focused then stemming should be used since lemmatizers scan a corpus which consumed time and processing. If we are building a language application in which language is important we should use lemmatization as it uses a corpus to match root forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
